---
title: "Introduction to single-cell RNA-seq analysis"
output:
  html_document:
    toc: yes
    toc_float: true
    toc_depth: 3
    number_sections: true
    pandoc_args: ["--number-offset", "4"] # TOC will start at 5
    code_folding: show
    css: ../css/boxes.css
---

# Normalisation and Feature selection

```{r setup, include=FALSE, purl=FALSE}
# Move all the course files to the `course_files` folder
# This setup makes sure the working directory is set to this folder
# All paths are relative to it, so participant scripts work when we purl it
knitr::opts_chunk$set(echo = TRUE,
                      root.dir = here::here("course_files"))
knitr::opts_knit$set(root.dir = here::here("course_files"))
set.seed(123)
```

Normalisation is a crucial step in single-cell RNA-seq data analysis. 
It aims to remove technical variations while preserving biological differences between cells. 
Various methods exist for normalising single-cell RNA-seq data, each with its own advantages and disadvantages.

## Setup

We will be using the following packages:

- `Seurat` for the main analysis
- `sctransform` for normalisation, with the additional `glmGamPoi` package, which makes it run faster
- `tidyverse` for data manipulation and plotting
- We also import specific functions from the `SparseArray` package for calculating summary statistics from matrices (we avoid importing the entire package to avoid conflicts with  functions in the other packages)

```{r load_libraries, message=FALSE, warning=FALSE}
# Load the libraries we will need for this practical
library(Seurat)
library(sctransform)
library(glmGamPoi)
library(tidyverse)
use("SparseArray", c("rowMeans", "rowVars", "colMeans", "colVars"))

# set default ggplot theme
theme_set(theme_classic())
```

For the purposes of this demonstration we have generated a smaller data set in
which there are only 500 cells per sample. This is so that the code can be run
in a reasonable amount of time during the live teaching session. The data were
first QC'd and filtered as described in the [QC and exploratory analysis 
session](04_Preprocessing_And_QC.html) using thresholds set on a per sample basis. After this 500 cells were selected at
random from each sample.

```{r load_data}
# Read preprocessed and filtered data object
seurat_object <- readRDS("RObjects/Filtered.500.rds")
seurat_object
```

We will demonstrate the effect of normalisation using a single sample to start with, with analysis of multi-sample data discussed later. 

```{r}
# Subset seurat object to include only PBMMC-1 sample for demonstration purposes
seurat_pbmmc1 <- subset(seurat_object,
                        subset = SampleName == "PBMMC-1")
seurat_pbmmc1
```

  
## Why normalise?

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r include=FALSE}

#### Why normalise? ####
```

The issue of normalisation is well illustrated by looking at the distribution of total UMI counts per cell. 

```{r norm_intro_plots}
# Plot the total UMI counts per cell for the PBMMC-1 sample before normalisation
seurat_pbmmc1@meta.data %>%
  # add the cell names as a column
  rownames_to_column("Cell") %>%
  # reorder the cells by their total UMI counts for plotting
  mutate(Cell = fct_reorder(Cell, nCount_RNA)) %>%
  # make a bar plot of the total UMI counts per cell
  ggplot(aes(x = Cell, y = nCount_RNA)) +
  geom_col() +
  scale_y_log10() +
  labs(x = "Cell",
       y = "Total cell UMI counts (log scale)",
       title = "PBMMC-1: Before Normalization") +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(color = "firebrick")
  )
```  

* The plot above shows the UMI counts/cell (transcript molecules) for 500 cell from the PBMMC-1 sample
* UMI counts fluctuate
* We derive biological insights downstream by comparing cells against each other.
* But the UMI counts differences makes it harder to compare cells.

* Why total transcript molecules (UMI counts) detected between cells differ?
  * Biological:
    * Cell sub type differences, like size and transcription activity etc.
  * Technical: scRNA data is inherently noisy
    * Low mRNA content per cell
    * cell-to-cell differences in mRNA capture efficiency
    * Variable sequencing depth
    * PCR amplification efficiency

* A normalization technique makes the UMI counts distribution uniform, so that each cell has similar counts.
* Normalization is a critical step that corrects cell-to-cell technical differences.
* By normalizing, downstream comparisons of relative expression between cells are valid.

## Normalization strategies

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r include=FALSE}

#### Exploring count normalisation ####
```


The sparse nature of scRNA data makes normalization difficult, unlike bulk RNAseq data.

* Broadly two classes

  1. Spike-in methods
      * Uses spike-in controls for normalisation
      * Not available for droplet based scRNA techniques like 10x.
      
  2.  Non-spike-in methods: 
      * Using available counts data for normalization
      * DEseq2
      * edgeR - TMM
      * Library size normalization
      * deconvolution
      * sctransform 
    
* Typical normalization has two steps
  1. scaling
      * Estimate size or scaling or normalization factor: computation of a cell-specific 'scaling' or 'size' factor or “normalization factor” that represents the relative bias in
that cell and division of all counts for the cell by that factor to remove that
bias. Assumption: any cell specific bias will affect genes the same way.
      * Scale the data by dividing the count for each gene with the appropriate size factor for that cell
  2. Transformation
      * log2
      * Square root transformation
      * Pearson residuals (eg. sctransform)


Scaling methods typically generate normalised counts-per-million (CPM) or
transcripts-per-million (TPM) values that address the effect of sequencing
depth. These values however typically have a variance that increases with their
mean (heteroscedasticity) while most statistical methods assume a stable
variance, which does not vary with the mean (homoscedasticity). A widely used
'variance stabilising transformation' is the log transformation (often log2).
This works well for highly expressed genes (as in bulk RNA-seq) but less so for sparse scRNA-seq data.


![DESeq,edgeR and Library size normalizations](../Images/size_factors_plot.png)


* DEseq, edgeR-TMM and Library size normalization initially developed for bulk RNAseq
* Applying these methods on scRNAseq data systematically under or over estimate size factors. i.e systematically deviate from true size factors.
* This deviation is the result of removing zeroes prior to normalization. 
* Therefore other normalization methods specific to scRNAseq data like deconvolution, sctransform etc. were proposed.

## sctransform: Variant Stabilising Transformation

The standard normalization method in Seurat is a global-scaling normalization method, which transforms the feature counts for each cell by dividing by the total counts of the cell and multiplying by a scale factor (default = 10000) and then applying a natural log with a pseudocount of 1. 
Global-scaling methods rely on an assumption that each cell originally contains the same number of RNA molecules. 

With scaling normalisations a correlation remains between the mean and variation
of expression (heteroskedasticity). This affects downstream dimensionality
reduction as the few main new dimensions are usually correlated with library
size. `sctransform` addresses the issue by regressing library size out of raw
counts and providing residuals to use as normalized and variance-stabilized
expression values in some downstream analyses, such as dimensionality reduction. The idea is to assign greater weight to lowly expressed genes that may exhibit cell type specific expression rather than highly expressed genes with broad expression.

!["Effect of scaling normalization"](../Images/total_UMI_counts_vs_gene_UMI_counts.png)

### Selection of highly variable genes

In feature selection the principle is to remove those genes which are uninteresting or uninformative to both improve computation and because this 'noise' reduction will hopefully enable us to more clearly see the true biology in our data. We make the assumption that most of the low level variance is not caused by real biology and is due to stochastic sampling in the single cell protocol and various other technical effects. The genes which have the most variance are therefore the ones that reflect the real biological difference and are what we want to focus on. This is obviously not a perfect assumption but it is a good way to make informed interpretations from your data that you can hopefully have a little more confidence in.

We often use scRNA-seq data in exploratory analyses to characterize
heterogeneity across cells. Procedures like clustering and dimensionality
reduction compare cells based on their gene expression profiles, which involves
aggregating per-gene differences into a single (dis)similarity metric between
every pair of cells. The choice of genes to use in this calculation has a major
impact on the behavior of the metric and the performance of downstream methods.
We want to select genes that contain useful information about the biology of the
system while removing genes that contain random noise. This aims to preserve
interesting biological structure without the variance that obscures that
structure, and to reduce the size of the data to improve computational
efficiency of later steps.

The simplest approach to feature selection is to select the most variable genes
based on their expression across the population. This assumes that genuine
biological differences will manifest as increased variation in the affected
genes, compared to other genes that are only affected by technical noise or a
baseline level of “uninteresting” biological variation (e.g., from
transcriptional bursting). Several methods are available to quantify the
variation per gene and to select an appropriate set of highly variable genes
(HVGs).

The use of the SCTransform replaces the need for finding highly variable features directly as it by default choose the top 3000 features by residual variance. We can change this when we run SCTransform using the `variable.features.n` argument or the `variable.features` argument to specify exactly which genes. We should check to ensure 3000 genes is the correct number for our data.

Assuming that, for most genes, the observed variance across cells is due to
technical noise, we can assess technical variation by fitting a trend line
between the mean-variance relationship across all genes. Genes that
substantially deviate from this trend may then be considered as highly-variable,
i.e. capturing biologically interesting variation.

Since in `Seurat` this sits outside the SCTransform methodology we will need to follow their standard global normalisation method to have a look at the genes. We will create a separate object for this investigation.

```{r normalise}
# Use the default normalisation method
# where the counts of each gene are divided by
# the total counts in the cell and log-transformed
seurat_pbmmc1 <- NormalizeData(seurat_pbmmc1)
```

We can now use the `FindVariableFeatures` function to identify highly variable genes (HVGs) from the normalised data. This identifies features that are outliers on a 'mean variability plot'. We have to select a method to choose the top features.

The three available methods are:

* `vst`: First, fits a line to the relationship of log(variance) and log(mean) using local polynomial regression (loess). Then standardizes the feature values using the observed mean and expected variance (given by the fitted line). Feature variance is then calculated on the standardized values after clipping to a maximum (see clip.max parameter).

* `mean.var.plot` (mvp): First, uses a function to calculate average expression (mean.function) and dispersion (dispersion.function) for each feature. Next, divides features into num.bin (default 20) bins based on their average expression, and calculates z-scores for dispersion within each bin. The purpose of this is to identify variable features while controlling for the strong relationship between variability and average expression.

* `dispersion` (disp): selects the genes with the highest dispersion values.

We will use the variance stabilising transform (`vst`) method.

Commonly used ways to decide on how many variable genes to select is to use a fixed number (e.g. 1000) or a proportion (e.g. the top 10%).

```{r 1000genes}
# Extract the 1000 most variable genes
# using the variance stabilising transformation method
seurat_pbmmc1 <- FindVariableFeatures(
  seurat_pbmmc1,
  selection.method = "vst",
  nfeatures = 1000
)
VariableFeaturePlot(seurat_pbmmc1)
```

```{r 10percent}
# Extract the top 10% most variable genes
seurat_pbmmc1 <- FindVariableFeatures(
  seurat_pbmmc1,
  selection.method = "vst",
  nfeatures = round(0.1 * nrow(seurat_pbmmc1))
)
VariableFeaturePlot(seurat_pbmmc1)
```

The function's default is to select 2000 features, but we will finally set this to 3000 to match what would be default in `sctransform`.

```{r HVGs}
# Extract the 3000 most variable genes to match the default in sctransform
seurat_pbmmc1 <- FindVariableFeatures(
  seurat_pbmmc1,
  selection.method = "vst",
  nfeatures = 3000
)
VariableFeaturePlot(seurat_pbmmc1)
```

The plot above looks like this is a reasonable number of genes to capture the biological variation rather than technical noise. If we want these specific genes we can pass them to SCTransform via the `residual.features` argument rather than allowing it to pick it's 3000 as the methods are slightly different. It is unlikely to make a big difference for most datasets unless you need specific genes to be included for a biological reason.

<!-- 
NOTE: Commented this out for now, as this is not showing what we might have expected. If you order the points by "top_3000" then we see that they are not all falling above the trendline, which is counter-intuitive. Did not have time to investigate this further, so will leave it out for now, but we should check this at some point.

It would be useful to view the trendline created in FindVariableFeatures to see how well the genes you have selected deviate from the trend. This is not directly available from the Seurat object so we will need to recreate it. We will continue to the last step of the normalisation, the scaling.

```{r recreate_trendline, message=FALSE, warning=FALSE}
# Scale the data to have mean 0 and variance 1 for each gene
seurat_pbmmc1 <- ScaleData(
  seurat_pbmmc1,
  features = rownames(seurat_pbmmc1),
  verbose = FALSE
)

# Get the normalized data
norm_data <- GetAssayData(seurat_pbmmc1, layer = "data")

# Calculate mean and variance for each gene
gene_means <- rowMeans(norm_data)
gene_vars <- rowVars(norm_data)

# Create a data frame
mean_var_df <- data.frame(gene = names(gene_means),
                          mean = gene_means,
                          variance = gene_vars) %>%
  # remove non-variable genes
  filter(variance > 0) %>%
  # log-transform mean and variance
  mutate(mean = log10(mean),
         variance = log10(variance))

# Add a trendline using loess regression
mean_var_df$trendline <- predict(
  loess(variance ~ mean, data = mean_var_df)
)

# Plot the mean-variance relationship with the trend line
ggplot(mean_var_df, aes(x = mean, y = variance)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(y = trendline), color = "firebrick", linewidth = 1) +
  labs(title = "Mean-Variance Relationship with Trend Line",
       x = "Mean Expression (log10)",
       y = "Variance (log10)")
```

We can check this against the 3000 selected above by colouring the plot.

```{r trendline_top3000}
mean_var_df <- mean_var_df %>%
  mutate(top_3000 = gene %in% VariableFeatures(seurat_pbmmc1)) |> 
  arrange(top_3000)

ggplot(mean_var_df, aes(x = mean, y = variance)) +
  geom_point(alpha = 0.3, aes(colour = top_3000)) +
  geom_line(aes(y = trendline), color = "firebrick", linewidth = 1) +
  labs(
    title = "Mean-Variance Relationship with Trend Line",
    x = "Mean Expression (log10)",
    y = "Variance (log10)"
  )
```

-->

### Run sctransform normalization

Transformed data will be available in the SCT assay, which is set as the default assay after running sctransform. During normalization, we can also remove confounding sources of variation, for example, mitochondrial mapping percentage. If you think think differences in mitochondrial gene expression are related to biology in your data, you may not want to do this, as it could help with clustering downstream.

```{r sctransform}
# Run sctransform normalisation
# regressing out the percentage of mitochondrial gene expression
seurat_pbmmc1 <- SCTransform(
  seurat_pbmmc1,
  vars.to.regress = "percent.mt",
  verbose = FALSE
)
```

### Explore the effect of sctransform normalisation

We can compare the effect of the normalisation on the counts values by plotting them as we did previously. The variance has been stabilised across the cells.

```{r sctransform_plots}
# plot that compares the sctransform normalised to the raw counts
raw_cts <- colSums(GetAssayData(seurat_pbmmc1, assay = "RNA", layer = "counts"))
raw_cts <- data.frame(cell = names(raw_cts),
                      counts = raw_cts)

log_cts <- colSums(GetAssayData(seurat_pbmmc1, assay = "RNA", layer = "data"))
log_cts <- data.frame(cell = names(log_cts),
                      counts = log_cts)

sct_cts <- colSums(GetAssayData(seurat_pbmmc1, assay = "SCT", layer = "counts"))
sct_cts <- data.frame(cell = names(sct_cts),
                      counts = sct_cts)

# Plot the total UMI counts per cell for the PBMMC-1 sample before normalisation
p_before_norm <- raw_cts %>%
  # reorder the cells by their total UMI counts for plotting
  mutate(Cell = fct_reorder(cell, counts)) %>%
  # make a bar plot of the total UMI counts per cell
  ggplot(aes(x = Cell, y = counts)) +
  geom_col() +
  # scale_y_log10() +
  labs(x = "Cell",
       y = "Total cell UMI counts (log scale)",
       title = "Before Normalization") +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(color = "firebrick")
  )

p_log_norm <- log_cts %>%
  # reorder the cells by their total UMI counts for plotting
  mutate(Cell = fct_reorder(cell, counts)) %>%
  # make a bar plot of the total UMI counts per cell
  ggplot(aes(x = Cell, y = counts)) +
  geom_col() +
  # scale_y_log10() +
  labs(x = "Cell",
       y = "Total cell UMI counts (log scale)",
       title = "Log Normalization") +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(color = "firebrick")
  )

p_after_norm <- sct_cts %>%
  # reorder the cells by their total UMI counts for plotting
  mutate(Cell = fct_reorder(cell, counts)) %>%
  # make a bar plot of the total UMI counts per cell
  ggplot(aes(x = Cell, y = counts)) +
  geom_col() +
  # scale_y_log10() +
  labs(x = "Cell",
       y = "Total cell UMI counts (log scale)",
       title = "SCTransform") +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(color = "firebrick")
  )

library(patchwork)
p_before_norm + p_log_norm + p_after_norm
```

## Working with multiple samples

When working with multiple samples, it is recommended to run the normalisation for each sample separately, so the model learns the technical variation within each sample and removes it. 

This is done by performing a so-called **split** of the RNA assay:

```{r split, eval=FALSE, purl=FALSE}
# Split the RNA assay by sample name
seurat_object[["RNA"]] <- split(seurat_object[["RNA"]],
                                f = seurat_object$SampleName)
```

The option `f` defines the factor by which we want to split the assay. 
In this case we're using the individual samples, but you could use other factors of our choice.

After this, when we run `SCTransform()`, it will apply the normalisation per batch. 

```{r sctransform_split, eval=FALSE, purl=FALSE}
# Run sctransform normalisation on the split assay
seurat_object <- SCTransform(
  seurat_object,
  assay = "RNA",
  vars.to.regress = "percent.mt",
  verbose = FALSE
)
```

An you could then continue with the downstream analysis, such as dimensionality reduction, which will be covered next. 

We will discuss more about splitting and integration in the [Data Integration](07_Dataset_Integration.html) section.

----

<details><summary>`sessionInfo()`</summary>

```{r session_info, echo=FALSE, purl=FALSE}
sessionInfo()
```

</details>
