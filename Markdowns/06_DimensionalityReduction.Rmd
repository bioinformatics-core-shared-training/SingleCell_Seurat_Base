---
title: "Introduction to single-cell RNA-seq analysis"
output:
  html_document:
    toc: yes
    toc_float: true
    toc_depth: 3
    number_sections: true
    pandoc_args: ["--number-offset", "5"] # TOC will start at 6
    code_folding: show
    css: ../css/boxes.css
---

# Dimensionality Reduction

```{r setup, include=FALSE, purl=FALSE}
# Move all the course files to the `course_files` folder
# This setup makes sure the working directory is set to this folder
# All paths are relative to it, so participant scripts work when we purl it
knitr::opts_chunk$set(echo = TRUE,
                      root.dir = here::here("course_files"))
knitr::opts_knit$set(root.dir = here::here("course_files"))
set.seed(123)
```

In this section we are going to cover the basics of 
dimensionality reduction. These methods allow us to represent our 
multi-dimensional data (with thousands of cells) in a 
reduced set of dimensions for visualisation and more efficient downstream 
analysis. 

In dimensionality reduction we attempt to find a way to represent all the the information we have in expression space in a way that we can easily interpret. High dimensional data has several issues. There is a high computational requirement to performing analysis on 30,000 genes and 47,000 cells (in the Caron dataset). Humans live in a 3D world; we can't easily visualise all the dimensions. And then there is sparsity. As the number of dimensions increases the distance between two data points increases and becomes more invariant. This invariance causes us problems when we try to cluster the data into biologically similar groupings. By applying some dimensionality reducing methods we can solve these issues to a level where we can make interpretations.

## Setup

Before we start, let's load our packages and read our data in. 

```{r load_libraries, message=FALSE, warning=FALSE}
# Load the libraries we will need for this practical
library(Seurat)
library(tidyverse)

# set default ggplot theme
theme_set(theme_classic())
```

We will load the _Seurat_ object generated in the [Normalisation](05_NormalisationAndFeatureSelection.html) section, which contains normalised counts for 500 cells per sample. 
For demonstration purposes we are not using the full dataset but you would in your analyses.

```{r load_data}
# load the preprocessed Seurat object with 500 cells per sample
seurat_object <- readRDS("RObjects/SCT.500.rds")
```

## Dimensionality reduction overview

Many scRNA-seq analysis procedures involve comparing cells based on their expression values across thousands of genes. 
Thus, each individual gene represents a dimension of the data (and the total number of genes represents the "dimensionality" of the data). 
More intuitively, if we had a scRNA-seq data set with only two genes, we could visualise our data in a two-dimensional scatterplot, with each axis representing the expression of a gene and each point in the plot representing a cell. 
Intuitively, we can imagine the same for 3 genes, represented as a 3D plot. 
Although it becomes harder to imagine, this concept can be extended to data sets with thousands of genes (dimensions), where each cell's expression profile defines its location in the high-dimensional expression space.

As the name suggests, dimensionality reduction aims to reduce the number of separate dimensions in the data. 
This is possible because different genes are correlated if they are affected by the same biological process. 
Thus, we do not need to store separate information for individual genes, but can instead compress multiple features into a single dimension, e.g., an “eigengene” (Langfelder and Horvath 2007). 
This reduces computational work in downstream analyses like clustering, as calculations only need to be performed for a few dimensions, rather than thousands. 
It also reduces noise by averaging across multiple genes to obtain a more precise representation of the patterns in the data. 
And finally it enables effective visualisation of the data, for those of
us who are not capable of visualizing more than 2 or 3 dimensions.

Here, we will cover three methods that are most commonly used in scRNA-seq
analysis:

- Principal Components Analysis (PCA)
- t-Distributed Stochastic Neighbor Embedding (t-SNE)
- Uniform Manifold Approximation and Projection (UMAP)

Before we go into the details of each method, it is important to mention that while the first method (PCA) can be used for downstream analysis of the data (such as cell clustering), the latter two methods (t-SNE and UMAP) should only be used for visualisation and not for any other kind of analysis.

## Principal Components Analysis

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r include=FALSE}

#### PCA ####
```

One of the most used and well-known methods of dimensionality reduction is
principal components analysis (PCA). This method performs a linear
transformation of the data, such that a set of variables (genes) are turned into
new variables called Principal Components (PCs).
These principal components combine information across several genes in a way
that best captures the variability observed across samples (cells).

Watch the video linked below for more details of how PCA works: 

<a href="https://www.youtube.com/embed/FgakZw6K1QQ" target="_blank">
<img src="https://img.youtube.com/vi/FgakZw6K1QQ/0.jpg" alt="StatQuest: PCA" height="200">
</a>

After performing a PCA, there is no data loss, i.e. the total number of
variables does not change. Only the fraction of variance captured by each
variable differs.

Each PC represents a dimension in the new expression space.
The first PC explains the highest proportion of variance possible.
The second PC explains the highest proportion of variance not explained by the
first PC.
And so on: successive PCs each explain a decreasing amount of variance not
captured by the previous ones.

The advantage of using PCA is that the total amount of variance explained by the
first few PCs is usually enough to capture most of the signal in the data.
Therefore, we can exclude the remaining PCs without much loss of information.
The stronger the correlation between the initial variables, the stronger the
reduction in dimensionality. We will see below how we can choose how many PCs to
retain for our downstream analysis.

### Running PCA

As discussed in the [Preprocessing and QC](04_Preprocessing_And_QC.html)
section, _Seurat_ objects contain a _slot_ that can store
representations of our data in reduced dimensions. This is useful as we can keep
all the information about our single-cell data within a single object.

The `RunPCA()` function can be used to run PCA on a seurat object, and returns an
updated version of the single cell object with the PCA result added to the
`reductions` slot.

By default the `RunPCA()` function will only use the variable features identified in the SCTransform step.

```{r run_pca }
# Run PCA
seurat_object <- RunPCA(seurat_object, 
                        features = VariableFeatures(seurat_object))
```

Running this function will add a new slot to our Seurat object called `pca` which contains the PCA representation of our data. This can be accessed using the `Reductions()` function.

```{r check_pca}
# check the new reductions slot
Reductions(seurat_object)
```

The output above tells us changes in which genes are represented by each principle component. One of the first things to investigate after doing a PCA is how much variance is explained by each PC. We can check the percentage of variance explained by each PC using the `Stdev()` function.

```{r check_pca_variance}
# check the variance explained by each PC
Stdev(seurat_object, reduction = "pca")
```

The typical way to view this information is using what is known as a "scree plot". In Seurat the function is called `ElbowPlot()`, and it plots the standard deviation of each PC, which is a measure of how much variance is explained by that PC. The more variance explained, the more important that PC is for representing the data. We can use this plot to decide how many PCs to retain for downstream analysis.

```{r scree_plot}
# plot the variance explained by each PC
ElbowPlot(seurat_object, ndims = 50)
```


We can see how the two first PCs explain a substantial amount of the variance,
and very little variation is explained beyond 10-15 PCs.

To visualise our cells in the reduced dimension space defined by PC1 and PC2, we
can use the `DimPlot()` function.

```{r plotPCA}
# plot the PCA
DimPlot(seurat_object,
        reduction = "pca")
```

The proximity of cells in this plot reflects the similarity of their expression
profiles. 

We can also plot different PCs, using the `dims` option:

```{r plotPCA_multiple}
# plot PC2 and PC3
DimPlot(seurat_object,
        reduction = "pca",
        dims = c(2, 3))
```

We can also split the plot by different options using the `split.by` option. For example, we can split the plot by sample group.

```{r plotPCA_split}
# plot PC1 and PC2 split by sample group
DimPlot(seurat_object,
        reduction = "pca",
        split.by = "SampleGroup")
```

## t-SNE: t-Distributed Stochastic Neighbor Embedding

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r include=FALSE}

#### t-SNE ####
```

The t-Distributed Stochastic Neighbor Embedding (t-SNE) approach addresses the
main shortcoming of PCA, which is that it can only capture linear
transformations of the original variables (genes). Instead, t-SNE allows for
non-linear transformations, while preserving the local structure of the data.
This means that neighbourhoods of similar samples (cells, in our case) will
appear together in a t-SNE projection, with distinct cell clusters separating
from each other.

As you can see below, compared to PCA we get much tighter clusters of samples,
with more structure in the data captured by two t-SNE axis, compared to the two
first principal components.

We will not go into the details of the algorithm here, but briefly it involves
two main steps:

- Calculating a similarity matrix between every pair of samples. This similarity
is scaled by a _Normal_ distribution, such that points that are far away from
each other are "penalised" with a very low similarity score. The variance of
this normal distribution can be thought of as a "neighbourhood" size when
computing similarities between cells, and is parameterised by a term called
**perplexity**.

- Then, samples are projected on a low-dimensional space (usually two
dimensions) such that the similarities between the points in this new space are
as close as possible to the similarities in the original high-dimensional space.
This step involves a stochastic algorithm that "moves" the points around until
it converges on a stable solution. In this case, the similarity between samples
is scaled by a t-distribution (that's where the "t" in "t-SNE" comes from),
which is used instead of the _Normal_ to guarantee that points within a cluster
are still distinguishable from each other in the 2D-plane (the t-distribution
has "fatter" tails than the Normal distribution).

Watch this video if you want to learn more about how t-SNE works:

<a href="https://www.youtube.com/embed/NEaUSP4YerM" target="_blank">
<img src="https://img.youtube.com/vi/NEaUSP4YerM/0.jpg" alt="StatQuest: t-SNE" height="200">
</a>

There are two important points to remember:

- the **perplexity** parameter, which indicates the relative importance of the
local and global patterns in the structure of the data set. The default value in
the functions we will use is 50, but different values should be tested to ensure
consistency in the results.
- **stochasticity**: because the t-SNE algorithm is stochastic, running the
analysis multiple times will produce slightly different results each time
(unless we set a "seed" for the random number generator).

See this interactive article on "[How to Use t-SNE
Effectively](https://distill.pub/2016/misread-tsne/)", which illustrates how
changing these parameters can lead to widely different results.

Importantly, because of the non-linear nature of this algorithm, strong
interpretations based on how distant different groups of cells are from each
other on a t-SNE plot are discouraged, as they are not necessarily meaningful.
This is why it is often the case that the x- and y-axis scales are omitted from
these plots (as in the example above), as they are largely uninterpretable.
Therefore, **the results of a t-SNE projection should be used for visualisation
only and not for downstream analysis (such as cell clustering)**.

### Running t-SNE

Similarly to how we did with PCA, there are functions that can run a t-SNE
directly on our SingleCellExperiment object. 
We will leave this exploration for you to do in the following exercises, but the basic code is very similar to that used with PCA. 
For example, the following would run t-SNE with default options:

```{r run_tsne}
# run t-SNE using default options
seurat_object <- RunTSNE(seurat_object,
                         reduction = "pca")

# confirm a new reduction was added to the object
Reductions(seurat_object)

# plot the t-SNE
DimPlot(seurat_object,
        reduction = "tsne")
```

Notice by default it colours using the `orig.ident` column in the meta.data. 

### Exercise: t-SNE

:::exercise

We want to achieve the following:

- Add a t-SNE projection of the data to our seurat object
- Explore how the stochasticity of the algorithm affects the results
- Customise our visualisation to display gene expression on top of the t-SNE
- Explore how the main tuneable parameter of the algorithm - perplexity -
affects the results

##### Run t-SNE {-}

The code below shows how you can:

1. Add the t-SNE result to the `Reductions` slot of the Seurat object. 
2. Give this dimensionality reduction a custom name - "TSNE_perplex30".
3. Set a perplexity value of our choice.
4. Use a chosen number of principal components.
5. Set a seed for the random number generator to ensure reproducibility of the results.

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r include=FALSE}

#### Exercise: t-SNE ####

# the following code:
# adds the t-SNE result to the Seurat object
# names this reduction "TSNE_perplex30"
# sets perplexity = 30 (which is the default if we don't specify it)
# uses the first 10 principal components
# sets a seed for reproducibility
```

```{r exercise1, results="hide"}
seurat_object <- RunTSNE(seurat_object,
                         reduction = "pca",
                         dims = 1:10,
                         perplexity = 30,
                         seed = 123,
                         reduction.name = "TSNE_perplex30",
                         reduction.key = "TSNE30_")
DimPlot(seurat_object,
        reduction = "TSNE_perplex30")
```


##### Part A {-}

* Re-run the algorithm but change the random seed number and generate a plot of 
the new reduction.   
* Do the results change dramatically between runs?

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r exercise1a, include=FALSE}
# Part A
# Re-run the algorithm but change the random seed number.
# Do the results change dramatically between runs?
# YOUR CODE HERE
```

<details><summary>Answer</summary>

```{r answer1a, purl=FALSE}
# re-run the t-SNE with a different seed
seurat_object <- RunTSNE(seurat_object,
                         reduction = "pca",
                         dims = 1:10,
                         perplexity = 30,
                         seed = 321,
                         reduction.name = "TSNE_perplex30_seed321",
                         reduction.key = "TSNE30seed321_")

# plot the new t-SNE
DimPlot(seurat_object, 
        reduction = "TSNE_perplex30_seed321")
```

</details>


##### Part B {-}

Facet these plots by SampleName to better understand where each marker is mostly expressed

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r exercise1b, include=FALSE}
# Part B
# Facet these plots by SampleName to better understand where each marker is mostly expressed
# YOUR CODE HERE
```

<details><summary>Hint</summary>

The function `DimPlot()` has an option called `split.by` that allows you to facet the plot by any of the columns in the meta.data slot of your seurat object. You can use this to facet by SampleName. It's best to specify the number of columns in the facet using the `ncol` option to make the plot easier to read.

</details>

<details><summary>Answer</summary>

```{r answer1b, purl=FALSE}
# plot the t-SNE faceted by SampleName
DimPlot(seurat_object,
        reduction = "TSNE_perplex30_seed321",
        split.by = "SampleName",
        ncol = 4)
```

</details>

##### Part C {-}

Rerun the t-SNE using different perplexity values (for example 5 and 500).
Name those reductions as "TSNE_perplex5" and "TSNE_perplex500" respectively.

* Do you get tighter or looser clusters?

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r exercise1c, include=FALSE}
# Explore different perplexity values (for example 5 and 500)
# Do you get tighter or looser clusters?
# YOUR CODE HERE
```

<details><summary>Answer</summary>

First re-run the t-SNE with different perplexity levels (running at 500 make take a little time)

```{r answer1c1, purl=FALSE}
# re-run the t-SNE with perplexity 5
seurat_object <- RunTSNE(seurat_object,
                         reduction = "pca",
                         dims = 1:10,
                         perplexity = 5,
                         seed = 321,
                         reduction.name = "TSNE_perplex5",
                         reduction.key = "TSNE5_")
# re-run the t-SNE with perplexity 500
seurat_object <- RunTSNE(seurat_object,
                         reduction = "pca",
                         dims = 1:10,
                         perplexity = 500,
                         seed = 321,
                         reduction.name = "TSNE_perplex500",
                         reduction.key = "TSNE500_")

# visualise all projections using DimPlot
DimPlot(seurat_object, reduction = "TSNE_perplex5")
DimPlot(seurat_object, reduction = "TSNE_perplex30")
DimPlot(seurat_object, reduction = "TSNE_perplex500")
```

</details>

##### Part D {-}

Instead of colouring by *SampleName* we can colour by expression of known cell markers by using the FeaturePlot() function. 

* CD79A (B cells)
* CST3 (monocytes)
* CD3D (T cells) 
* HBA1 (erythrocytes)

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r exercise1d, include=FALSE}
# Instead of colouring by SampleName, colour by expression of known cell markers
# CD79A (B cells)
# CST3 (monocytes)
# CD3D (T cells)
# HBA1 (erythrocytes)
# YOUR CODE HERE
```

<details><summary>Hint</summary>

You can replace what we colour by with any of the gene names in our dataset 
as they are stored as the rownames in our object. Look at the FeaturePlot() help page to find the correct argument to use.

</details>

<details><summary>Answer</summary>

E.g for CD79A:

```{r answer1d, purl=FALSE}
# colour tsne by CD79A expression
FeaturePlot(seurat_object,
            features = "CD79A",
            reduction = "TSNE_perplex30")
```

Some things to note from our data exploration: 

- Changing the random seed doesn't seem to _qualitatively_ change the results
dramatically. The same groups of cells are still seen. However, their relative
position on the axis might change from run to run.
- Low perplexities will favour resolution of finer structure, possibly to the
point that the visualization is compromised by random noise. Thus, it is
advisable to test different perplexity values to ensure that the choice of
perplexity does not drive the interpretation of the plot.
- Exploring the expression of known marker genes (from literature or previous
experiments we might have done) may help us interpret these plots and judge
whether a particular choice of perplexity is adequate to represent the expected
cell types in our data.

</details>

:::

## UMAP: Uniform Manifold Approximation and Projection

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r include=FALSE}

#### UMAP ####
```

Simiarly to t-SNE, UMAP performs a non-linear transformation of the data to
project it down to lower dimensions.
One difference to t-SNE is that this method claims to preserve both local and
global structures (i.e. the relative positions of clusters are, most of the
times, meaningful).
However, it is worth mentioning that there is some debate as to whether this is
always the case, as is explored in this recent paper by [Chari, Banerjee and
Pachter (2021)](https://doi.org/10.1101/2021.08.25.457696).

Compared to t-SNE, the UMAP visualization tends to have more compact visual
clusters with more empty space between them. It also attempts to preserve more
of the global structure than t -SNE. From a practical perspective, UMAP is much
faster than t-SNE, which may be an important consideration for large datasets.

Similarly to t-SNE, since this is a non-linear method of dimensionality
reduction, **the results of a UMAP projection should be used for visualisation
only and not for downstream analysis (such as cell clustering)**.

### Running UMAP

Running UMAP is very similar to what we've seen already for PCA and t-SNE, only the function name changes:

```{r runUMAP, warning=FALSE}
# run UMAP using the first 10 PCs
seurat_object <- RunUMAP(seurat_object,
                         reduction = "pca",
                         dims = 1:10)

# confirm a new reduction was added to the object
Reductions(seurat_object)

# visualise the UMAP
DimPlot(seurat_object,
        reduction = "umap")
```

Because this UMAP also involves a series of randomization steps, setting the
random-generator seed (as we did above) is critical if we want to obtain
reproducible results after each run.

Like t-SNE, UMAP has its own suite of hyperparameters that affect the
visualization. Of these, the number of neighbors (n_neighbors) and the minimum
distance between embedded points (min_dist) have the greatest effect on the
granularity of the output. If these values are too low, random noise will be
incorrectly treated as high-resolution structure, while values that are too high
will discard fine structure altogether in favor of obtaining an accurate
overview of the entire dataset. Again, it is a good idea to test a range of
values for these parameters to ensure that they do not compromise any
conclusions drawn from a UMAP plot.

See this interactive article that goes into more depth about the underlying
methods, and explores the impacts of changing the n_neighbours and min_dist
parameters:
[Understanding UMAP](https://pair-code.github.io/understanding-umap/).

Similarly to what we did with t-SNE, we will explore this further in the
following exercise.

### Exercise: UMAP

:::exercise

Our main objectives are: 

- Add a UMAP projection of the data to our seurat object
- Explore how the main tuneable parameter of the algorithm - neighbourhood size - affects the results
- Compare how UMAP compares to t-SNE in terms of revealing structure in the data

##### Part A {-}

- Run the UMAP with 30 neighbours (look at the help page for the `RunUMAP()` function to find the correct argument to use).
- Use the first 10 PCs as input.
- Set a seed for reproducibility.
- Give this dimensionality reduction a custom name - "UMAP_neighbors30".

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r exercise2a, include=FALSE}

#### Exercise: UMAP ####

# Part A
# run the UMAP with 50 neighbours
# YOUR CODE HERE
```

<details><summary>Answer</summary>

```{r exercise2_solutionA, purl=FALSE}
# run the UMAP with 30 neighbours
seurat_object <- RunUMAP(seurat_object, 
                         reduction = "pca", 
                         dims = 1:10,
                         seed.use = 123,
                         n.neighbors = 30,
                         reduction.name = "UMAP_neighbors30",
                         reduction.key = "UMAPneighbors30_")
```

</details>


##### Part B {-}

Now visualise the resulting UMAP projection.

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r exercise2b, include=FALSE}
# Part B
# visualise the resulting UMAP projection
# YOUR CODE HERE
```

<details><summary>Answer</summary>

```{r exercise2_solutionB, purl=FALSE}
# visualise the resulting UMAP projection
DimPlot(seurat_object,
        reduction = "UMAP_neighbors30")
```

</details>

##### Part C {-}

Run the UMAP with 5 and 500 neighbours and compare the results. 
Similarly to the TSNE, running with higher neighbour values will take more time.

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r exercise2c, include=FALSE}
# Part C
# run the UMAP with 5 and 500 neighbours and compare the results
# YOUR CODE HERE
```

<details><summary>Answer</summary>

```{r exercise2_solutionC, purl=FALSE}
# run the UMAP with 5 neighbours
seurat_object <- RunUMAP(seurat_object, 
                         reduction = "pca", 
                         dims = 1:10,
                         seed.use = 123,
                         n.neighbors = 5,
                         reduction.name = "UMAP_neighbors5",
                         reduction.key = "UMAPneighbors5_")
# run the UMAP with 500 neighbours
seurat_object <- RunUMAP(seurat_object, 
                         reduction = "pca", 
                         dims = 1:10,
                         seed.use = 123,
                         n.neighbors = 500,
                         reduction.name = "UMAP_neighbors500",
                         reduction.key = "UMAPneighbors500_")

# visualise all projections using DimPlot
DimPlot(seurat_object, reduction = "UMAP_neighbors5")
DimPlot(seurat_object, reduction = "UMAP_neighbors30")
DimPlot(seurat_object, reduction = "UMAP_neighbors500")
```

</details>

##### Part D {-}

* Compare the UMAP projection with the t-SNE projections 
* Would you prefer one over the other?

<!-- Note: this code chunk is here so we purl it into the participant script -->
```{r exercise2d, include=FALSE}
# Part D
# compare the UMAP projection with the t-SNE projections
# would you prefer one over the other?
# YOUR CODE HERE
```

<details><summary>Answer</summary>

```{r exercise2_solutionD, purl=FALSE}
# plot the two projections
DimPlot(seurat_object, reduction = "TSNE_perplex30")
DimPlot(seurat_object, reduction = "UMAP_neighbors30")
```

To answer some of those questions:

- Increasing the number of neighbours used in the analysis results in some
apparent groups of cells being "merged" with each other. Conversely, picking too
small a size creates many groups of cells, possibly picking up on some noise.
However, qualitatively the results are relatively robust to the number of
neighbours in this dataset, with the major groups of cells consistently
appearing together.
- Both t-SNE and UMAP show similar patterns in the data, so which one to pick in this case seems to be a matter of personal preference. However, UMAP is a more modern algorithm with better properties when it comes to keeping relative
positions between clusters.

</details>
:::


## Key Points and Resources

:::highlight

**Key Points:**

- Dimensionality reduction methods allow us to represent high-dimensional data
in lower dimensions, while retaining biological signal.
- The most common methods used in scRNA-seq analysis are PCA, t-SNE and UMAP.
- PCA uses a linear transformation of the data, which aims at defining new
dimensions (axis) that capture most of the variance observed in the original
data. This allows to reduce the dimension of our data from thousands of genes to
10-20 principal components.
- The results of PCA can be used in downstream analysis such as cell clustering,
trajectory analysis and even as input to non-linear dimensionality reduction
methods such as t-SNE and UMAP.
- t-SNE and UMAP are both non-linear methods of dimensionality reduction. They
aim at keeping similar cells together and dissimilar clusters of cells apart
from each other.
- Because these methods are non-linear, they should only be used for data
visualisation, and not for downstream analysis.

:::

----

<details><summary>`sessionInfo()`</summary>

```{r session_info, echo=FALSE, purl=FALSE}
sessionInfo()
```

</details>


